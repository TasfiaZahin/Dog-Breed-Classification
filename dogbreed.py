# -*- coding: utf-8 -*-
"""DogBreed

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N8j1G1_uWKcl91dydiAqLpKAXlteT3R_
"""

from google.colab import drive
drive.mount('/content/gdrive')

!unzip "/content/gdrive/My Drive/Colab Notebooks/Kaggle/Data.zip" -d "/content/"

import numpy as np
import math
import pandas as pd

seed_value = 0
np.random.seed(seed_value)

import tensorflow as tf
print(tf.__version__)
from tensorflow import keras
from keras.layers import Input, Dense
from keras.models import Model, Sequential
from keras.layers import Dense, Conv2D, Dropout, BatchNormalization, Input, Reshape, Flatten, Conv2DTranspose, MaxPooling2D, UpSampling2D
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam
from keras.callbacks import Callback

from tensorflow.keras import initializers
import matplotlib.pyplot as plt
from tqdm import tqdm
import cv2 # image handling

import sklearn
from sklearn.model_selection import train_test_split

tf.random.set_seed(seed_value)

# 1. Set `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# 2. Set `python` built-in pseudo-random generator at a fixed value
import random
random.seed(seed_value)

# 3. Configure a new global `tensorflow` session
from keras import backend as K

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

class EarlyStoppingByLossVal(Callback):
    def __init__(self, monitor='loss', value=0.00001, verbose=0, patience=50):
        super(Callback, self).__init__()
        self.monitor = monitor
        self.value = value
        self.verbose = verbose
        self.patience = patience

    def on_epoch_end(self, epoch, logs={}):
        current = logs.get(self.monitor)
        # print("Current Value:")
        # print(current)
        if current is None:
            warnings.warn("Early stopping requires %s available!" % self.monitor, RuntimeWarning)

        if current < self.value:
            if self.verbose > 0:
                print("Epoch %05d: early stopping THR" % epoch)
            self.model.stop_training = True

from keras.backend import sigmoid
from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation

class Swish(Activation):  
  def __init__(self, activation, **kwargs):
      super(Swish, self).__init__(activation, **kwargs)
      self.__name__ = 'swish'

def swish(x, beta = 1):
  return (x * sigmoid(beta * x))

get_custom_objects().update({'swish': Swish(swish)})

from keras.backend import sigmoid
from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation

class Swish(Activation):  
  def __init__(self, activation, **kwargs):
      super(Swish, self).__init__(activation, **kwargs)
      self.__name__ = 'swish'

def swish(x, beta = 1):
  return (x * sigmoid(beta * x))

get_custom_objects().update({'swish': Swish(swish)})

fileName = "/content/Data/labels.csv"
df = pd.read_csv(fileName)

label_map = df.to_numpy()
print(label_map)

labels = label_map[:,1]
#print(labels)

names = np.unique(label_map[:,1])
names.sort()
print(len(names))
print(names)

breeds = len(names) 

label_dict = {} #dict with image id and label
for val in label_map:
  label_dict[val[0]] = val[1]
print(label_dict)

#print(len(label_map))
#print(len(label_dict))

onehot_dict = {} #dict with breed name and onehot, onehot has 1 in index of sorted names array

for i in range(len(names)):
  name = names[i]
  arr = np.zeros(len(names))
  arr[i] = 1
  onehot_dict[name] = arr

print(len(onehot_dict))

#variables

stop_threshold = 0.001
stop_epoch = 20

img_rows=128
img_cols=128
channel=3 # 3 colour channes

#loading the images

x_feature = []
y_feature = []

i = 0 # initialisation
for id, name in tqdm(df.values): # f is id, name is string label
  train_img = cv2.imread('/content/Data/train/{}.jpg'.format(id),1) #0 means grey imaged loaded, 1 means color image loaded
  #print(train_img.shape)
  label = onehot_dict[name]
  train_img_resize = cv2.resize(train_img, (img_rows, img_cols))
  x_feature.append(train_img_resize)
  y_feature.append(label)
  i += 1

#x_feature = np.array(x_feature)
#y_feature = np.array(y_feature)
#print(x_feature.shape,y_feature.shape)

x_train_data = np.array(x_feature, np.float32)/255.0 # /= 255 for normalisation
# x_train_data = np.expand_dims(x_train_data, axis = 3) #expand to 4 dim, last one is colour channel (batch,height,width,channel)
print (x_train_data.shape)

y_train_data = np.array(y_feature, np.uint8)
print (y_train_data.shape)

x_train, x_val, y_train, y_val = train_test_split(x_train_data, y_train_data, test_size=0.2, random_state=0)
print (x_train.shape)
print (x_val.shape)

#cnn layers

inp = Input((img_rows, img_cols, channel),)

e = Conv2D(64, (4,4), activation="relu", padding = 'Same')(inp)
e = BatchNormalization()(e)
e = MaxPooling2D((2, 2))(e)
e = Conv2D(64, (4,4), activation="relu", padding = 'Same')(e)
e = BatchNormalization()(e)
e = MaxPooling2D((2, 2))(e)
e = Conv2D(64, (4,4), activation="relu", padding = 'Same')(e)
e = BatchNormalization()(e)
e = MaxPooling2D((2, 2))(e)
e = Flatten()(e)
e = Dense(breeds, activation="relu")(e)
output = Dense(breeds, activation='sigmoid')(e)

classifier = Model(inp, output)
print(classifier.summary())

opt = keras.optimizers.Adam(learning_rate=0.01)
classifier.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])

callbacks = [
    EarlyStoppingByLossVal(monitor='loss', value=stop_threshold, verbose=1, patience=50)
    # EarlyStopping(monitor='val_loss', patience=2, verbose=0),
    #ModelCheckpoint(kfold_weights_path, monitor='val_loss', save_best_only=True, verbose=0),
]

history = classifier.fit(x_train, y_train,
                epochs=stop_epoch,
                verbose=2,
                batch_size=100,
                shuffle=True,
                validation_data=(x_val, y_val),
                callbacks=callbacks)
  
fig, ax = plt.subplots(2,1)
ax[0].plot(history.history['loss'], color='g', label="Training loss")
ax[0].plot(history.history['val_loss'], color='b', label="validation loss",axes =ax[0])
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['accuracy'], color='g', label="Training accuracy")
ax[1].plot(history.history['val_accuracy'], color='b',label="Validation accuracy")
legend = ax[1].legend(loc='best', shadow=True)

# saving whole model
modelFileName = "/content/drive/My Drive/Colab Notebooks/Kaggle"
classifier.save(modelFileName)
print("Model saved.")

# loading and preparing test dataset

fileName = "/content/Data/sample_submission.csv"
sub = pd.read_csv(fileName)
test_ids = sub['id']
print (test_ids[0])

x_test_feature = []

i = 0 # initialization
for id in tqdm(test_ids.values): # f is id, name is string label
  test_img = cv2.imread('/content/Data/test/{}.jpg'.format(id),1) #0 means grey imaged loaded, 1 means color image loaded
  test_img_resize = cv2.resize(test_img, (img_rows, img_cols))
  x_test_feature.append(test_img_resize)
  i += 1

x_test = np.array(x_test_feature, np.float32)/255.0 
print (x_test.shape)

# loading whole model

# modelFileName = "/content/drive/My Drive/Colab Notebooks/Kaggle"

# from keras.models import load_model
# classifier = load_model(modelFileName)

output_vector = classifier.predict(x_test)
print(output_vector)

output_dict = {}

import operator

i=0
for id in test_ids.values:
  index, value = max(enumerate(output_vector[i]), key=operator.itemgetter(1))
  #print(index)
  i += 1
  output_dict[id] = names[index]

print(output_dict)